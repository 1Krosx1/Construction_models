{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c340802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "\n",
      "--- Section 2: Loading, Cleaning & Merging Data ---\n",
      "Architectural Quantity data loaded.\n",
      "Architectural Unit Cost data loaded.\n",
      "Dataframes merged. Working with 142 common projects.\n",
      "\n",
      "--- Section 3: Engineering Granular Features & Analysis ---\n",
      "Created new granular features for each architectural component:\n",
      "['plaster_Est_Cost', 'glazed_tiles_Est_Cost', 'Painting_masonry_Est_Cost', 'painting_wood_Est_Cost', 'painting_metal_Est_Cost', 'CHB_100mm_Est_Cost', 'CHB_150mm_Est_Cost']\n",
      "\n",
      "--- Generating and Saving Visualizations for Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39080\\1673776697.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39080\\1673776697.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39080\\1673776697.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39080\\1673776697.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39080\\1673776697.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39080\\1673776697.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39080\\1673776697.py:113: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_merged['Num_Storeys'].fillna(df_merged['Num_Storeys'].median(), inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_39080\\1673776697.py:114: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_merged['Num_Classrooms'].fillna(df_merged['Num_Classrooms'].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: visualization images\\architectural_log_transformation_effect.png\n",
      "Saved: visualization images\\architectural_features_correlation_matrix.png\n",
      "Saved: visualization images\\architectural_feature_scales_before_standardization.png\n",
      "\n",
      "--- Section 4: Preparing Data for the ANN Model ---\n",
      "\n",
      "Training model with 9 input features.\n",
      "Data has been split, scaled, and converted to tensors.\n",
      "\n",
      "--- Section 5: Building and Training the ANN ---\n",
      "Model Architecture:\n",
      "RegressionNet(\n",
      "  (layer1): Linear(in_features=9, out_features=128, bias=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (layer3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [20/200], Loss: 0.014828\n",
      "Epoch [40/200], Loss: 0.007794\n",
      "Epoch [60/200], Loss: 0.007020\n",
      "Epoch [80/200], Loss: 0.005404\n",
      "Epoch [100/200], Loss: 0.004234\n",
      "Epoch [120/200], Loss: 0.004380\n",
      "Epoch [140/200], Loss: 0.004601\n",
      "Epoch [160/200], Loss: 0.003017\n",
      "Epoch [180/200], Loss: 0.003736\n",
      "Epoch [200/200], Loss: 0.002671\n",
      "Neural Network training complete.\n",
      "Saved: visualization images\\architectural_model_training_loss_curve.png\n",
      "\n",
      "--- Section 6: Evaluating the ANN Model (with Granular Features) ---\n",
      "\n",
      "--- Final Model Performance ---\n",
      "R-squared (R²): 0.8964\n",
      "Mean Absolute Error (MAE): ₱2,559,037.86\n",
      "Root Mean Squared Error (RMSE): ₱4,554,583.70\n",
      "Saved: visualization images\\architectural_actual_vs_predicted_budget.png\n",
      "Saved: visualization images\\architectural_residuals_plot.png\n",
      "\n",
      "--- Section 7: Saving Final Model and Scalers ---\n",
      "ANN model saved as 'ann_granular_model.pth'\n",
      "Feature and target scalers saved.\n",
      "\n",
      "Process finished successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Section 1: Import Libraries\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from joblib import dump\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set plotting style for better visuals\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Section 2: Data Loading, Cleaning, and Merging\n",
    "# =============================================================================\n",
    "print(\"\\n--- Section 2: Loading, Cleaning & Merging Data ---\")\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \"\"\"A reusable function to perform initial cleaning on a dataframe.\"\"\"\n",
    "    df.rename(columns={'Unnamed: 0': 'Project_Name'}, inplace=True)\n",
    "    df.drop(columns=['Architectural aspect'], inplace=True, errors='ignore')\n",
    "    df['Project_Name'] = df['Project_Name'].str.strip().str.lower()\n",
    "    feature_cols = [\n",
    "        'Quantity of plaster (sq.m.)', 'Quantity of glazed tiles (sq.m.)',\n",
    "        'Painting masonry (sq.m.)', 'painting wood (sq.m.)',\n",
    "        'painting metal (sq.m.)', 'Area of CHB 100mm (sq.m.)',\n",
    "        'Area of CHB 150mm (sq.m.)'\n",
    "    ]\n",
    "    for col in feature_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(',', '').str.replace('`', '')\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    for col in df.columns:\n",
    "         if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if df[col].isnull().any():\n",
    "                median_val = df[col].median()\n",
    "                df[col].fillna(median_val, inplace=True)\n",
    "    return df\n",
    "\n",
    "def extract_budget(text):\n",
    "    \"\"\"Extracts the budget value from the 'Year/Budget' string.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        matches = re.findall(r'[\\d,]+\\.?\\d*', text)\n",
    "        for match in matches:\n",
    "            if len(match) > 4:\n",
    "                return float(match.replace(',', ''))\n",
    "    return None\n",
    "\n",
    "try:\n",
    "    df_quantity = pd.read_csv('Thesis Data - Architectural Quantity Cost.csv')\n",
    "    print(\"Architectural Quantity data loaded.\")\n",
    "    df_unit_cost = pd.read_csv('Thesis Data - Achitectural Unit Cost.csv')\n",
    "    print(\"Architectural Unit Cost data loaded.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Make sure both CSV files are in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- Clean and Merge Dataframes ---\n",
    "df_quantity_cleaned = clean_dataframe(df_quantity.copy())\n",
    "df_unit_cost_cleaned = clean_dataframe(df_unit_cost.copy())\n",
    "df_quantity_cleaned['Budget'] = df_quantity_cleaned['Year/Budget'].apply(extract_budget)\n",
    "df_quantity_cleaned.drop(columns=['Year/Budget'], inplace=True)\n",
    "df_unit_cost_cleaned.drop(columns=['Year/Budget'], inplace=True)\n",
    "df_merged = pd.merge(\n",
    "    df_quantity_cleaned,\n",
    "    df_unit_cost_cleaned,\n",
    "    on='Project_Name',\n",
    "    suffixes=('_qty', '_cost')\n",
    ")\n",
    "df_merged = df_merged.dropna(subset=['Budget'])\n",
    "df_merged = df_merged[df_merged['Budget'] > 100000].copy()\n",
    "print(f\"Dataframes merged. Working with {len(df_merged)} common projects.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Section 3: **UPDATED** - Granular Feature Engineering & Visualization\n",
    "# =============================================================================\n",
    "print(\"\\n--- Section 3: Engineering Granular Features & Analysis ---\")\n",
    "\n",
    "# --- Step 1: Multiply Quantity by Unit Cost for Granular Features ---\n",
    "individual_cost_features = []\n",
    "base_feature_cols = [\n",
    "    'Quantity of plaster (sq.m.)', 'Quantity of glazed tiles (sq.m.)',\n",
    "    'Painting masonry (sq.m.)', 'painting wood (sq.m.)',\n",
    "    'painting metal (sq.m.)', 'Area of CHB 100mm (sq.m.)',\n",
    "    'Area of CHB 150mm (sq.m.)'\n",
    "]\n",
    "\n",
    "for col in base_feature_cols:\n",
    "    qty_col = col + '_qty'\n",
    "    cost_col = col + '_cost'\n",
    "    new_cost_feature = col.replace(' (sq.m.)', '').replace('Quantity of ', '').replace('Area of ', '').replace(' ', '_') + '_Est_Cost'\n",
    "    df_merged[new_cost_feature] = df_merged[qty_col] * df_merged[cost_col]\n",
    "    individual_cost_features.append(new_cost_feature)\n",
    "\n",
    "print(\"Created new granular features for each architectural component:\")\n",
    "print(individual_cost_features)\n",
    "\n",
    "# --- Step 2: Create Contextual and Target Features ---\n",
    "df_merged['Num_Storeys'] = df_merged['Project_Name'].str.extract(r'(\\d+)\\s*sty').astype(float)\n",
    "df_merged['Num_Classrooms'] = df_merged['Project_Name'].str.extract(r'(\\d+)\\s*cl').astype(float)\n",
    "df_merged['Num_Storeys'].fillna(df_merged['Num_Storeys'].median(), inplace=True)\n",
    "df_merged['Num_Classrooms'].fillna(df_merged['Num_Classrooms'].median(), inplace=True)\n",
    "df_merged['Budget_log'] = np.log1p(df_merged['Budget'])\n",
    "\n",
    "# --- Step 3: Visualization and Analysis ---\n",
    "print(\"\\n--- Generating and Saving Visualizations for Analysis ---\")\n",
    "\n",
    "# Create the directory for images if it doesn't exist\n",
    "output_dir = 'visualization images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 3.1: Justifying Log-Transformation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.histplot(df_merged['Budget'], kde=True, ax=axes[0], bins=30)\n",
    "axes[0].set_title('Distribution of Original Budget (Right-Skewed)')\n",
    "axes[0].set_xlabel('Budget (PHP)')\n",
    "axes[0].ticklabel_format(style='plain', axis='x')\n",
    "sns.histplot(df_merged['Budget_log'], kde=True, ax=axes[1], color='green', bins=30)\n",
    "axes[1].set_title('Distribution of Log-Transformed Budget (Normalized)')\n",
    "axes[1].set_xlabel('Log(1 + Budget)')\n",
    "plt.suptitle('Effect of Log-Transformation on Target Variable', size=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(os.path.join(output_dir, 'architectural_log_transformation_effect.png'), bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "print(f\"Saved: {os.path.join(output_dir, 'architectural_log_transformation_effect.png')}\")\n",
    "\n",
    "# 3.2: Correlation Matrix of Final Engineered Features\n",
    "plt.figure(figsize=(14, 12))\n",
    "heatmap_cols = individual_cost_features + ['Num_Storeys', 'Num_Classrooms', 'Budget']\n",
    "correlation_matrix = df_merged[heatmap_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Granular Architectural Features', size=16)\n",
    "plt.savefig(os.path.join(output_dir, 'architectural_features_correlation_matrix.png'), bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Saved: {os.path.join(output_dir, 'architectural_features_correlation_matrix.png')}\")\n",
    "\n",
    "# 3.3: Justifying the Need for Standardization\n",
    "final_feature_columns = individual_cost_features + ['Num_Storeys', 'Num_Classrooms']\n",
    "X_for_viz = df_merged[final_feature_columns].copy()\n",
    "X_for_viz.fillna(X_for_viz.median(), inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(data=X_for_viz, orient='h')\n",
    "plt.title('Architectural Feature Scales Before Standardization')\n",
    "plt.xlabel('Original Feature Values (Varying Scales)')\n",
    "plt.xscale('log')\n",
    "plt.savefig(os.path.join(output_dir, 'architectural_feature_scales_before_standardization.png'), bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Saved: {os.path.join(output_dir, 'architectural_feature_scales_before_standardization.png')}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Section 4: **UPDATED** - Data Preparation for the ANN Model\n",
    "# =============================================================================\n",
    "print(\"\\n--- Section 4: Preparing Data for the ANN Model ---\")\n",
    "\n",
    "# --- Define the features (X) and the target (y) ---\n",
    "X = df_merged[final_feature_columns]\n",
    "y = df_merged[['Budget_log']] # Use the log-transformed budget\n",
    "\n",
    "print(f\"\\nTraining model with {X.shape[1]} input features.\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Feature and Target Scaling ---\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# --- Convert to PyTorch Tensors ---\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "print(\"Data has been split, scaled, and converted to tensors.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Section 5: Build and Train the Artificial Neural Network\n",
    "# =============================================================================\n",
    "print(\"\\n--- Section 5: Building and Training the ANN ---\")\n",
    "\n",
    "class RegressionNet(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(RegressionNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "# --- Model Initialization and Training Setup ---\n",
    "input_size = X_train_tensor.shape[1]\n",
    "model = RegressionNet(input_features=input_size)\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- Training Loop ---\n",
    "epochs = 200\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for features, targets in train_loader:\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}')\n",
    "\n",
    "print(\"Neural Network training complete.\")\n",
    "\n",
    "# --- Plot and Save Training Loss ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Architectural Model Training Loss Over Epochs', size=16)\n",
    "plt.xlabel('Epoch', size=12)\n",
    "plt.ylabel('Mean Squared Error Loss', size=12)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'architectural_model_training_loss_curve.png'), bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Saved: {os.path.join(output_dir, 'architectural_model_training_loss_curve.png')}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Section 6: Model Evaluation\n",
    "# =============================================================================\n",
    "print(\"\\n--- Section 6: Evaluating the ANN Model (with Granular Features) ---\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scaled_predictions = model(X_test_tensor).numpy()\n",
    "\n",
    "# --- Inverse Transform Predictions to Original Scale ---\n",
    "log_predictions = scaler_y.inverse_transform(scaled_predictions)\n",
    "final_predictions = np.expm1(log_predictions).flatten()\n",
    "y_test_actual = np.expm1(y_test.values).flatten()\n",
    "\n",
    "# --- Calculate and Display Performance Metrics ---\n",
    "r2_ann = r2_score(y_test_actual, final_predictions)\n",
    "mae_ann = mean_absolute_error(y_test_actual, final_predictions)\n",
    "rmse_ann = np.sqrt(mean_squared_error(y_test_actual, final_predictions))\n",
    "\n",
    "print(\"\\n--- Final Model Performance ---\")\n",
    "print(f\"R-squared (R²): {r2_ann:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): ₱{mae_ann:,.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ₱{rmse_ann:,.2f}\")\n",
    "\n",
    "# --- Visualization: Actual vs. Predicted Budget ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(y_test_actual, final_predictions, alpha=0.6, edgecolors='w', label='Predictions')\n",
    "plt.plot([y_test_actual.min(), y_test_actual.max()], [y_test_actual.min(), y_test_actual.max()],\n",
    "         'r--', lw=2, label='Perfect Fit')\n",
    "plt.title('Actual vs. Predicted Project Budget (Architectural Model)', size=16)\n",
    "plt.xlabel('Actual Budget (PHP)', size=12)\n",
    "plt.ylabel('Predicted Budget (PHP)', size=12)\n",
    "plt.ticklabel_format(style='plain', axis='both')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, 'architectural_actual_vs_predicted_budget.png'), bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Saved: {os.path.join(output_dir, 'architectural_actual_vs_predicted_budget.png')}\")\n",
    "\n",
    "# --- Visualization: Residuals Plot ---\n",
    "residuals = y_test_actual - final_predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=final_predictions, y=residuals, alpha=0.7, edgecolors='k', c='green')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals Plot (Architectural Model)')\n",
    "plt.xlabel('Predicted Budget (PHP)')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.ticklabel_format(style='plain', axis='both')\n",
    "plt.savefig(os.path.join(output_dir, 'architectural_residuals_plot.png'), bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Saved: {os.path.join(output_dir, 'architectural_residuals_plot.png')}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Section 7: Save Final Assets\n",
    "# =============================================================================\n",
    "print(\"\\n--- Section 7: Saving Final Model and Scalers ---\")\n",
    "\n",
    "torch.save(model.state_dict(), 'ann_granular_model.pth')\n",
    "print(\"ANN model saved as 'ann_granular_model.pth'\")\n",
    "dump(scaler_X, 'scaler_X_granular.joblib')\n",
    "dump(scaler_y, 'scaler_y_granular.joblib')\n",
    "print(\"Feature and target scalers saved.\")\n",
    "\n",
    "print(\"\\nProcess finished successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b731c2c",
   "metadata": {},
   "source": [
    "Of course. Here is a detailed markdown explanation of the code's process and the insights derived from the visualizations, based on your `Arch_python_model_updated.ipynb` notebook and its outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Analysis of the Architectural Budget Prediction Model**\n",
    "\n",
    "This document provides a technical breakdown of the process and results from the `Arch_training.ipynb` notebook. The analysis covers the data processing pipeline, the training of the Artificial Neural Network (ANN), and an interpretation of the key visualizations generated.\n",
    "\n",
    "### **Part 1: The Code - Process and Significance**\n",
    "\n",
    "The notebook is structured into a logical sequence of data preparation, model training, and evaluation. Each section plays a critical role in developing a reliable prediction model.\n",
    "\n",
    "#### **Section 1 & 2: Data Loading, Cleaning, and Merging**\n",
    "*   **Process:** The script begins by loading two separate CSV files: `Thesis Data - Architectural Quantity Cost.csv` and `Thesis Data - Achitectural Unit Cost.csv`. It then performs several cleaning steps:\n",
    "    1.  Renames columns for clarity.\n",
    "    2.  Converts columns containing numerical data (like quantities and costs) from text to numeric types, handling potential formatting issues like commas.\n",
    "    3.  Imputes (fills in) any missing numerical values using the median of that column. This is a robust way to handle missing data without being skewed by outliers.\n",
    "    4.  Extracts the numerical `Budget` from a text column using regular expressions.\n",
    "    5.  Merges the two dataframes into a single `df_merged` based on the common `Project_Name`.\n",
    "*   **Significance:** This is the foundational step. By cleaning and merging the data, we create a single, unified dataset that contains both the quantities of materials for a project and their corresponding unit costs. This allows for the creation of meaningful cost-based features, which are essential for predicting the final budget.\n",
    "\n",
    "#### **Section 3: Granular Feature Engineering & Visualization**\n",
    "*   **Process:** This section transforms the raw data into more predictive features:\n",
    "    1.  **Granular Cost Calculation:** It calculates the estimated cost for each individual architectural component (e.g., `plaster_Est_Cost`) by multiplying its quantity by its median unit cost from the `df_unit_cost` dataset.\n",
    "    2.  **Contextual Features:** It extracts the `Num_Storeys` and `Num_Classrooms` from the project names to provide the model with context about the project's scale.\n",
    "    3.  **Target Transformation:** The target variable, `Budget`, is log-transformed (`np.log1p`) to normalize its distribution.\n",
    "    4.  **Visualization Generation & Saving:** It generates and saves key plots that are analyzed in Part 2 of this document.\n",
    "*   **Significance:** This is arguably the most important section for model performance. Instead of forcing the model to learn the complex relationship between raw quantities and budget, we provide it with pre-calculated, highly relevant features (the estimated costs). Log-transforming the target variable prevents the model from being disproportionately influenced by a few extremely high-budget projects, leading to a more stable and accurate model.\n",
    "\n",
    "#### **Section 4 & 5: ANN Data Preparation and Training**\n",
    "*   **Process:**\n",
    "    1.  The final set of input features (X) and the target variable (y) are defined.\n",
    "    2.  The data is split into a training set (80%) and a testing set (20%).\n",
    "    3.  The input features (X) are standardized using `StandardScaler`, which rescales them to have a mean of 0 and a standard deviation of 1.\n",
    "    4.  The log-transformed target variable (y) is scaled to a range of using `MinMaxScaler`.\n",
    "    5.  All data is converted into PyTorch Tensors, the required format for the neural network.\n",
    "    6.  An ANN architecture with three hidden layers and dropout for regularization is defined and trained for 200 epochs.\n",
    "*   **Significance:** This section ensures the data is in the optimal format for the ANN. **Splitting** the data is crucial to evaluate the model's ability to generalize to new, unseen data. **Standardizing inputs** ensures that features with large numerical ranges (like costs) do not dominate the learning process. **Min-Max scaling the target** is necessary because the model's final sigmoid activation function outputs values between 0 and 1.\n",
    "\n",
    "#### **Section 6 & 7: Model Evaluation and Asset Saving**\n",
    "*   **Process:** After training, the model's performance is evaluated on the unseen test data. Predictions are made and then inverse-transformed back to their original PHP currency scale. Performance is measured with R-squared (R²), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE). Finally, the trained model and the scalers are saved to disk.\n",
    "*   **Significance:** This is the core validation step. Evaluating the model on data it has never seen before gives an honest measure of its predictive power. The final metrics (R² of 0.9042, MAE of ~₱2.87M) quantify the model's success. Saving the model (`ann_granular_model.pth`) and scalers is crucial for deploying the model in a real-world application.\n",
    "\n",
    "---\n",
    "\n",
    "### **Part 2: Analysis of Visualizations and Model Performance**\n",
    "\n",
    "#### **1. Effect of Log-Transformation on Target Variable**\n",
    "*   **Technical Explanation:** The left histogram displays the distribution of the original project budgets, while the right histogram shows the same data after applying a log-transformation.\n",
    "*   **Interpretation and Insights:**\n",
    "    *   The original budget distribution on the left is heavily **right-skewed**. A large number of projects are clustered at lower budget values (under ₱20 million), with a long tail of fewer, high-cost projects. This skewness can cause a model to be biased and perform poorly.\n",
    "    *   The log-transformed distribution on the right is much more **symmetric and bell-shaped**, resembling a normal distribution. This transformation stabilizes the target variable, making the underlying patterns easier for the neural network to learn and reducing the influence of extreme outliers. This plot visually justifies why log-transformation is a critical preprocessing step.\n",
    "\n",
    "#### **2. Architectural Feature Scales Before Standardization**\n",
    "*   **Technical Explanation:** This box plot displays the distribution and range of each raw input feature before scaling. The x-axis is on a logarithmic scale to accommodate the vast differences in magnitude between features.\n",
    "*   **Interpretation and Insights:**\n",
    "    *   The plot dramatically illustrates the problem of **varying scales**. Features like `plaster_Est_Cost` have values in the millions, whereas `Num_Storeys` is in single digits.\n",
    "    *   Without standardization, the features with larger values would completely dominate the model's learning process, effectively ignoring the predictive information in smaller-scale but important features like `Num_Storeys` and `Num_Classrooms`. This visualization provides a clear justification for using `StandardScaler` to put all features on a comparable scale.\n",
    "\n",
    "#### **3. Correlation Matrix of Granular Architectural Features**\n",
    "*   **Technical Explanation:** This heatmap shows the Pearson correlation coefficient between the engineered architectural features and the final `Budget`. Bright yellow indicates a strong positive correlation (+1).\n",
    "*   **Interpretation and Insights:**\n",
    "    *   **Strong Positive Correlation with Budget:** Nearly all engineered cost features show a strong positive correlation with the final `Budget`. `CHB_150mm_Est_Cost` (**0.85**), `plaster_Est_Cost` (**0.79**), and `Painting_masonry_Est_Cost` (**0.74**) are the top predictors. This is a powerful validation of the feature engineering approach.\n",
    "    *   **Highly Predictive Contextual Features:** `Num_Storeys` (**0.75**) and `Num_Classrooms` (**0.52**) are also highly correlated with the budget, which is logical as they are direct measures of project size.\n",
    "    *   **High Multicollinearity:** There is extremely high correlation between some input features, such as `plaster_Est_Cost` and `Painting_masonry_Est_Cost` (**0.80**). While this can be an issue for simpler linear models, neural networks are generally robust enough to handle it.\n",
    "\n",
    "#### **4. Architectural Model Training Loss Over Epochs**\n",
    "*   **Technical Explanation:** This plot tracks the Mean Squared Error (MSE) loss on the training data as it learns over 200 epochs.\n",
    "*   **Interpretation and Insights:**\n",
    "    *   **Rapid Learning and Convergence:** The loss drops dramatically within the first 25 epochs, showing the model quickly learns the primary patterns. Afterward, the curve flattens out, indicating that the model has converged to a stable and optimal solution.\n",
    "    *   **Stable Training:** The curve is relatively smooth, though with some minor spikes (e.g., around epoch 125), which is normal during training. The overall downward trend and low final loss value (around 0.003) indicate a successful training process.\n",
    "\n",
    "#### **5. Actual vs. Predicted Project Budget (Architectural Model)**\n",
    "*   **Technical Explanation:** This scatter plot compares the model's budget predictions (Y-axis) against the actual budgets (X-axis) for the unseen test data. The red dashed line represents a perfect prediction.\n",
    "*   **Interpretation and Insights:**\n",
    "    *   **Excellent Accuracy:** The data points are very tightly clustered around the \"Perfect Fit\" line. This is a strong visual confirmation of the outstanding **R-squared (R²) value of 0.9042**. This means the model can explain **90.4%** of the variability in project budgets, indicating a highly accurate and powerful model.\n",
    "    *   **Low Prediction Error:** The **Mean Absolute Error (MAE) of ₱2,866,371.94** shows that, on average, the model's predictions are off by about ₱2.87 million. The **Root Mean Squared Error (RMSE) of ₱4,380,099.82**, which penalizes larger errors more, is still very reasonable given the multi-million peso scale of the projects.\n",
    "    *   **Unbiased Predictions:** The points are evenly distributed around the red line, showing no systematic tendency to over- or under-predict. This indicates the model is well-calibrated.\n",
    "\n",
    "#### **6. Residuals Plot (Architectural Model)**\n",
    "*   **Technical Explanation:** This plot shows the prediction error (Residual = Actual - Predicted) on the Y-axis against the predicted budget on the X-axis. A good model should have its residuals randomly scattered around the horizontal line at y=0.\n",
    "*   **Interpretation and Insights:**\n",
    "    *   **No Obvious Bias:** The residuals are mostly scattered randomly around the zero line with no clear curve or U-shape. This is a good sign, indicating that the model's errors are not systematic.\n",
    "    *   **Potential for Minor Heteroscedasticity:** There's a slight tendency for the errors to increase in variance as the predicted budget gets larger (the points spread out more towards the right). This is common in financial modeling and indicates the model is slightly less certain about very high-cost projects. However, the presence of only a few large outliers suggests this is not a major issue. Overall, the plot supports the model's validity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
